{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting data ingestion, cleaning and transformation ...\n",
      "\n",
      "\n",
      "Floor Date: 2019-04-01 00:00:00\n",
      "Ceiling Date: 2020-04-12 00:00:00\n",
      "\n",
      "\n",
      "Data ingestion, cleaning and transformation completed in 0.64 seconds\n",
      "\n",
      "All data is stored in data > reference folder\n",
      "\n",
      "Locale data is stored in data > processed folder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time,os,re,csv,sys,xlrd,yaml,glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "class makeDatasetTs:\n",
    "    \n",
    "    \"\"\" \n",
    "    Data ingestion, cleaning and transformation for time series analysis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.path = config[\"project_path\"]\n",
    "        self.data_input = config[\"data_path\"][\"input\"]\n",
    "        self.data_output = config[\"data_path\"][\"output\"]\n",
    "        self.data_ref = config[\"data_path\"][\"ref\"]\n",
    "        self.report_img = config[\"reports\"][\"images\"]\n",
    "\n",
    "    def ingest_raw_dataset(self):   \n",
    "        \n",
    "        filepaths = glob.glob(os.path.join(self.path, self.data_input, \"*.xlsx\"))\n",
    "        all_data = pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            checksum = []\n",
    "            for f in glob.glob(os.path.join(self.path, self.data_input, \"*.xlsx\")):\n",
    "                df = pd.read_excel(f, sheet_name='Task Count')\n",
    "                checksum.append((f, len(df), sum(df['TaskCount'])))\n",
    "                all_data = all_data.append(df,ignore_index=True)\n",
    "\n",
    "            #save checksum\n",
    "            checksum = pd.DataFrame(checksum, columns = ['filename', 'row_count', 'TaskCountSum'])\n",
    "            checksum.to_csv(os.path.join(self.path, self.data_ref, \"metadata.csv\"), index=False)\n",
    "\n",
    "        except:\n",
    "            print(f\"Found problematic file: {f}\")\n",
    "            \n",
    "        return all_data\n",
    "    \n",
    "    \n",
    "    def clean_dataset(self, all_data):\n",
    "        \n",
    "        self.all_data = all_data\n",
    "        all_data = self.all_data\n",
    "        \n",
    "        #Convert dates from object to timestamps\n",
    "        all_data['TaskDate'] = all_data['TaskDate'].apply(lambda x: pd.Timestamp(x))\n",
    "        all_data['ReportDate'] = all_data['ReportDate'].apply(lambda x: pd.Timestamp(x))\n",
    "              \n",
    "        #Convert string dates in excel to timestamps - this will take a few minutes to run\n",
    "        rogue_list = [ '1970-01-01 00:00:00.0000' + str(rogue) for rogue in all_data['TaskDate'].unique().tolist() if len(str(rogue)) != 19]\n",
    "        for rl in rogue_list:\n",
    "\n",
    "            rogues = all_data[all_data['TaskDate'] == rl]\n",
    "            rogues_idx = rogues.index.tolist()\n",
    "\n",
    "            for idx in rogues_idx:\n",
    "                r_int = int(rogues['TaskDate'].astype('str')[idx][-5:])\n",
    "                r_int_2 = int(rogues['ReportDate'].astype('str')[idx][-5:])\n",
    "                all_data.loc[idx, 'TaskDate'] = datetime(*xlrd.xldate_as_tuple(r_int, 0))\n",
    "                all_data.loc[idx, 'ReportDate'] = datetime(*xlrd.xldate_as_tuple(r_int_2, 0))\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(f\"Floor Date: {min(all_data['TaskDate'])}\")\n",
    "        print(f\"Ceiling Date: {max(all_data['TaskDate'])}\\n\")\n",
    "        \n",
    "        all_data['EmailId']=pd.factorize(all_data['RaterEmail'])[0]+1\n",
    "        all_data = all_data.drop(['RaterEmail'], axis=1)\n",
    "        \n",
    "        all_data.to_csv(os.path.join(self.path, self.data_ref, \"all_data.csv\"), index=False)\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def select_dataset(self, all_data, freq = 'W'):\n",
    "        \n",
    "        self.all_data = all_data\n",
    "        self.freq = freq\n",
    "        all_data = self.all_data\n",
    "        \n",
    "        df_summary = pd.DataFrame(all_data.groupby([\"Language\"])['TaskCount'].sum()).reset_index()\n",
    "        df_summary['RaterCount'] = list(all_data.groupby('Language')['EmailId'].nunique())\n",
    "        df_summary['ProjectCount'] = list(all_data.groupby('Language')['Rater Visible Name'].nunique())\n",
    "        date_min_max = all_data.groupby('Language').agg({'TaskDate': [np.min,np.max]}).reset_index()['TaskDate'].rename(columns = {\"amin\": \"MinTaskDate\", \"amax\":\"MaxTaskDate\"})\n",
    "        df_summary = pd.concat([df_summary, date_min_max], axis=1)\n",
    "        df_summary = df_summary.sort_values(['TaskCount', 'RaterCount'], ascending=[False,False],ignore_index=True)\n",
    "        df_summary['Market'] = np.where(df_summary['MinTaskDate'].dt.year > 2020 , 'New', 'Current')\n",
    "\n",
    "        conditions = [\n",
    "            (df_summary['TaskCount'] >= 10000000),\n",
    "            ((df_summary['TaskCount'] < 10000000) & (df_summary['TaskCount'] >= 400000)),\n",
    "            (df_summary['TaskCount'] < 400000)]\n",
    "\n",
    "        choices = ['High','Medium','Low']\n",
    "        df_summary['Volume'] = np.select(conditions, choices, default=np.nan)\n",
    "        \n",
    "        sns.set(rc={\"figure.figsize\": (15, 8)})\n",
    "        sns.set_palette(reversed(sns.color_palette(\"Blues_d\", 45)), 45)\n",
    "        ax = sns.barplot(y = \"Language\", x= \"TaskCount\", data=df_summary, alpha=0.7)\n",
    "        ax.axhline(30, ls='--', color='r')\n",
    "        ax.text(200,31,\"Cut-off : Low volume and new market\", size=10, verticalalignment='center')\n",
    "        ax.set_title(\"Total task count by Language-Location\", size=15) \n",
    "        plt.savefig(os.path.join(self.path, self.report_img, \"selected_data.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        selected_data = df_summary[(df_summary['Market'] == 'Current') & (df_summary['Volume'] != 'Low')]\n",
    "        selected_data = selected_data['Language'].unique().tolist()\n",
    "        \n",
    "        for sd in selected_data:\n",
    "            \n",
    "            selection = all_data[all_data['Language'] == sd]\n",
    "            ts = selection[['TaskDate', 'TaskCount']].set_index('TaskDate').resample(self.freq).sum().reset_index()\n",
    "            ts.to_csv(os.path.join(self.path, self.data_output, \"ts_\" + sd + \".csv\"), index=False)\n",
    "        \n",
    "        return selected_data\n",
    "    \n",
    "def make_dataset():\n",
    "\n",
    "    # Function to load yaml configuration file\n",
    "    def load_config(config_name):\n",
    "        with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "\n",
    "        return config\n",
    "\n",
    "    config_path = \"conf/base\"\n",
    "\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    # Start data processing and transformation\n",
    "    \n",
    "    print(\"\\nStarting data ingestion, cleaning and transformation ...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    make_data = makeDatasetTs(config)\n",
    "    \n",
    "    all_data = make_data.ingest_raw_dataset()\n",
    "    cleaned_data = make_data.clean_dataset(all_data)\n",
    "    selected_data = make_data.select_dataset(all_data)\n",
    "    \n",
    "    #print(selected_data.head())\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    processing_time = round((end_time - start_time)/60,2)\n",
    "    \n",
    "    print(f\"\\nData ingestion, cleaning and transformation completed in {processing_time} minutes\")\n",
    "    \n",
    "    print(f\"\\nAll data is stored in data > reference folder\")\n",
    "    print(f\"\\nLocale data is stored in data > processed folder\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    make_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azt",
   "language": "python",
   "name": "azt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
